<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9"/>
<title>xvsdk: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">xvsdk
   &#160;<span id="projectnumber">3.2.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">xvsdk Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1>!! Draft version !!</h1>
<h2><a href="./annotated.html">C++ API Reference</a></h2>
<h2>Some conventions</h2>
<ul>
<li><b>physical units</b> are all in SI (International System of Units) by default (second, meter, radian ..), otherwise the name is suffixed with unit (example: <code>interEyesDistanceMm</code> if is in mm or <code>edgeTimestampUs</code> if timestamp in microseconds).</li>
<li><b>3D rotation:</b> we use right handed convention, recommand to use rotation matrix as many as possible or quaternions if need less memory usage. SDK provides tools for conversions between 3D rotation formats (see <a class="el" href="group__xv__rotation__conversions.html">functions to convert between 3D rotations representations</a>) and other conventions (left handed).</li>
<li><b>device frame coordinates</b> is based on the IMU frame coordinates with X to right, Y to down and Z to forward (main direction of the cameras).</li>
<li><b>world frame coordinates</b> is usually aligned with gravity (using the IMU) and X to right, Y to down and Z to forward of the initial pose.</li>
<li><b>components frame coordinates</b> (stereo cameras, ToF, color camera, AR/VR displays ...) are usually defined by calibration and are defined according to the device frame coordianates (IMU)</li>
<li><b>timestamps:</b> the API usually use the two following timestamps fields:<ul>
<li><code>double hostTimestamp</code> : host timestamp in seconds of the physical event of the data based on <code>std::chrono::steady_clock</code> that is guaranteed to be monotonic by the C++ STL</li>
<li><code>std::int64_t edgeTimestampUs</code> : timestamp of the physical event of the data based on edge clock</li>
</ul>
</li>
</ul>
<h2>Examples</h2>
<p>The main entry point to use a XVisio is <a class="el" href="classxv_1_1Device.html">xv::Device</a>. This class is the main entry point of the API, it gives access to the device and algorithms. See <a class="el" href="group__xv__functions.html#ga9d51997c35d0ab6d4cc2708d6b6b9ab0" title="Retrieve all the detected XVisio devices. If no device is found after the timeout is reached...">xv::getDevices()</a> to have an instance corresponding to a device.</p>
<p>A device can have multiple components, accessible with member functions :</p><ul>
<li><a class="el" href="classxv_1_1Device.html#a82af73bff459756055b16f4919621472" title="Get the SLAM component. ">xv::Device::slam()</a> : 6dof tracker doing the SLAM algorithm on host based on informations coming from device (stereo camera, IMU sensor, ..)</li>
<li><a class="el" href="classxv_1_1Device.html#aab6d916babd2f4f78d5f12c00847394f" title="Get the IMU sensor of the device. ">xv::Device::imuSensor()</a> : sensor with at least 3-axis accelerometer and 3-axis gyrometer</li>
<li><a class="el" href="classxv_1_1Device.html#a160863c661624997a5db4ffe20306bd8" title="Get the stereo cameras component of the device. ">xv::Device::fisheyeCameras()</a>: typically 2 fisheye cameras used for Visual SLAM</li>
<li><a class="el" href="classxv_1_1Device.html#aeab96f3c3a38b0771126a4fd9b1d7fe0" title="Get the ToF component of the device. ">xv::Device::tofCamera()</a>: a depth camera sensor</li>
<li>#xv::Device::edge(): used to run some algorithm directly on embedded device (typically Visual SLAM) when it is possible to choose between host and edge processing</li>
<li><a class="el" href="classxv_1_1Device.html#a088d0774607e803076cb889e70af2036" title="Get the display component. ">xv::Device::display()</a>: used to handle informations about the display (if device can display like a HMD)</li>
<li><a class="el" href="classxv_1_1Device.html#a16049ac3cef9f3035e45675a63dfaa8d" title="Get the object detection component. ">xv::Device::objectDetector()</a>: used to run and get results of the CNN object detector</li>
</ul>
<p>If a device does not support a component or doesn't have the component (for example ToF), the accessor function will return <code>null_ptr</code>. The data streams and processings under a component are activated only if at least one callback is registerd to the component. If all the callbacks are unregistered then steams can be deactivated.</p>
<p>Here are some example how to use the differents components.</p>
<h3>6dof tracking with SLAM on host and get fisheye images</h3>
<p>```cpp #include &lt;<a class="el" href="xv-sdk_8h_source.html">xv-sdk.h</a>&gt;</p>
<p>#include &lt;iostream&gt; #include &lt;thread&gt; #include &lt;atomic&gt; #include &lt;cmath&gt;</p>
<p>#include "frequency_counter.hpp"</p>
<p>void onPose(xv::Pose const&amp; pose){ </p><pre class="fragment">// use the conversion function to get the pitch, yaw and roll of the orientation
// (be carefull of gimbal lock of Euler angles, that is why Euler angles are not in Pose structure)
auto pitchYawRoll = xv::rotationToPitchYawRoll(pose.rotation());
static FrequencyCounter fps;
fps.tic();
if (fps.count() % 500 == 1) {
    std::cout &lt;&lt; "SLAM pose callback : " &lt;&lt; fps.fps() &lt;&lt; " Hz [timestamp=" &lt;&lt; pose.hostTimestamp() &lt;&lt; " x=" &lt;&lt; pose.x() &lt;&lt; " y=" &lt;&lt; pose.y() &lt;&lt; " z=" &lt;&lt; pose.z()
              &lt;&lt; " pitch="  &lt;&lt; pitchYawRoll[0]*180./M_PI &lt;&lt; "°" &lt;&lt; " yaw="  &lt;&lt; pitchYawRoll[1]*180./M_PI &lt;&lt; "°" &lt;&lt; " roll="  &lt;&lt; pitchYawRoll[2]*180./M_PI &lt;&lt; "°"
              &lt;&lt; std::endl;
}
</pre><p> }</p>
<p>std::shared_ptr&lt;const xv::FisheyeImages&gt; lastStereoImage;</p>
<p>int main( int /*argc*/, char* /*argv*/[] ) { </p><pre class="fragment">// may change the log level this way :
xv::setLogLevel(xv::LogLevel::debug);

// return a map of devices with serial number as key, wait at most 3 seconds if no device detected
auto devices = xv::getDevices(5.);

// if no device: quit
if (devices.empty()) {
    std::cerr &lt;&lt; "Timeout for device detection." &lt;&lt; std::endl;
    return EXIT_FAILURE;
}

// take the first device in the map
auto device = devices.begin()-&gt;second;

if (!device-&gt;slam()) {
    std::cerr &lt;&lt; "Host SLAM algorithm not supported." &lt;&lt; std::endl;
    return EXIT_FAILURE;
}

// register a callback to get the current pose based on SLAM, the callback is call at IMU framerate.
// Framerate can be high, for AR/VR device we recommand using `getPose` function to get the current 6dof pose with no latency.
device-&gt;slam()-&gt;registerPoseCallback(onPose);

// if access to full stereo camera images it is possible to get images via a callback :
if (device-&gt;fisheyeCameras()) {
    device-&gt;fisheyeCameras()-&gt;registerCallback([](std::shared_ptr&lt;const xv::FisheyeImages&gt; images) {
        lastStereoImage = images;
        static FrequencyCounter fps;
        fps.tic();
        if (fps.count() % 30 == 1) {
            std::cout &lt;&lt; "Fisheyes : " &lt;&lt; fps.fps() &lt;&lt; " Hz" &lt;&lt; std::endl;
        }
    });
}

// Simulate a 60Hz loop to show the use of the `getPose` function.
std::atomic&lt;bool&gt; stop(false);
std::thread threadLoop60Hz([&amp;stop, &amp;device]{

    while (!stop) {

        auto now = std::chrono::steady_clock::now();

        xv::Pose pose;
        // get the pose at current time (no latency because internally compensated with small prediction after the last IMU data received) ...
        if (device-&gt;slam()-&gt;getPose(pose)) {
            auto pitchYawRoll = xv::rotationToPitchYawRoll(pose.rotation());
            static FrequencyCounter fps;
            fps.tic();
            if (fps.count() % 120 == 1) {
                std::cout &lt;&lt; "Current SLAM : " &lt;&lt; fps.fps() &lt;&lt; " Hz [timestamp=" &lt;&lt; pose.hostTimestamp() &lt;&lt; " x=" &lt;&lt; pose.x() &lt;&lt; " y=" &lt;&lt; pose.y() &lt;&lt; " z=" &lt;&lt; pose.z()
                          &lt;&lt; " pitch="  &lt;&lt; pitchYawRoll[0]*180./M_PI &lt;&lt; "°" &lt;&lt; " yaw="  &lt;&lt; pitchYawRoll[1]*180./M_PI &lt;&lt; "°" &lt;&lt; " roll="  &lt;&lt; pitchYawRoll[2]*180./M_PI &lt;&lt; "°"
                          &lt;&lt; std::endl;
            }
        }

        // it is also possible to make prediction and to get the pose 30 ms in the future:
        xv::Pose pose30ms;
        device-&gt;slam()-&gt;getPose(pose30ms, 0.030);

        if (lastStereoImage) {
            // it is also possible to get the pose at a given time, for example the last stereo images time:
            xv::Pose poseAtStereoImage;
            device-&gt;slam()-&gt;getPoseAt(poseAtStereoImage, lastStereoImage-&gt;hostTimestamp);
        }

        // it is also possible to get the pose of the device at a specific timestamp (based on std::chrono::steady_clock)
        // the pose cannot be to old and not too much in the future
        double t = std::chrono::duration_cast&lt;std::chrono::seconds&gt;(std::chrono::steady_clock::now().time_since_epoch()).count();
        t += 0.0123;
        xv::Pose poseAt;
        device-&gt;slam()-&gt;getPoseAt(poseAt, t);

        // to simulate the 60Hz loop
        std::this_thread::sleep_until(now+std::chrono::microseconds(long(1./60.*1e6)));
    }

});

std::cout &lt;&lt; "Press enter to start SLAM ..." &lt;&lt; std::endl;
std::cin.get();

// start the SLAM
device-&gt;slam()-&gt;start();

std::cout &lt;&lt; "Press enter to stop SLAM ..." &lt;&lt; std::endl;
std::cin.get();

// stop the slam
device-&gt;slam()-&gt;stop();

stop = true;

if (threadLoop60Hz.joinable()) {
    threadLoop60Hz.join();
}

return EXIT_SUCCESS;
</pre><p> } ```</p>
<h3>Edge SLAM 6dof pose tracking and RGB camera output</h3>
<p>```cpp #include &lt;<a class="el" href="xv-sdk_8h_source.html">xv-sdk.h</a>&gt;</p>
<p>#include &lt;iostream&gt; #include &lt;thread&gt; #include &lt;atomic&gt; #include &lt;cmath&gt;</p>
<p>#include "frequency_counter.hpp"</p>
<p>void onPose(xv::Pose const&amp; pose){ </p><pre class="fragment">// use the conversion function to get the pitch, yaw and roll of the orientation
// (be carefull of gimbal lock of Euler angles, that is why Euler angles are not in Pose structure)
auto pitchYawRoll = xv::rotationToPitchYawRoll(pose.rotation());
static FrequencyCounter fps;
fps.tic();
if (fps.count() % 120 == 1) {
    std::cout &lt;&lt; "Current SLAM : " &lt;&lt; fps.fps() &lt;&lt; " Hz [timestamp=" &lt;&lt; pose.hostTimestamp() &lt;&lt; " x=" &lt;&lt; pose.x() &lt;&lt; " y=" &lt;&lt; pose.y() &lt;&lt; " z=" &lt;&lt; pose.z()
              &lt;&lt; " pitch="  &lt;&lt; pitchYawRoll[0]*180./M_PI &lt;&lt; "°" &lt;&lt; " yaw="  &lt;&lt; pitchYawRoll[1]*180./M_PI &lt;&lt; "°" &lt;&lt; " roll="  &lt;&lt; pitchYawRoll[2]*180./M_PI &lt;&lt; "°"
              &lt;&lt; std::endl;
}
</pre><p> }</p>
<p>std::shared_ptr&lt;const xv::ColorImage&gt; lastColorImage;</p>
<p>int main( int /*argc*/, char* /*argv*/[] ) { </p><pre class="fragment">// return a map of devices with serial number as key, wait at most 3 seconds if no device detected
auto devices = xv::getDevices(3.);

// if no device: quit
if (devices.empty()) {
    std::cerr &lt;&lt; "Timeout for device detection." &lt;&lt; std::endl;
    return EXIT_FAILURE;
}

// take the first device in the map
auto device = devices.begin()-&gt;second;

if (!device-&gt;edge()) {
    std::cerr &lt;&lt; "Edge SLAM algorithm not supported." &lt;&lt; std::endl;
    return EXIT_FAILURE;
}

// register a callback to get the current pose based on edge SLAM, the callback is call at IMU framerate.
device-&gt;edge()-&gt;registerPoseCallback(onPose);

// if access to full stereo camera images it is possible to get images via a callback :
if (device-&gt;colorCamera()) {
    device-&gt;colorCamera()-&gt;registerCallback([](std::shared_ptr&lt;const xv::ColorImage&gt; image) {
        lastColorImage = image;
        static FrequencyCounter fps;
        fps.tic();
        if (fps.count() % 120 == 1) {
            std::cout &lt;&lt; "Current color image : " &lt;&lt; fps.fps() &lt;&lt; " Hz [timestamp=" &lt;&lt; image-&gt;hostTimestamp &lt;&lt; " width=" &lt;&lt; image-&gt;width &lt;&lt; " height=" &lt;&lt; image-&gt;height
                      &lt;&lt; "]" &lt;&lt; std::endl;
        }
    });
} else {
    std::cerr &lt;&lt; "Access to color camera not supported." &lt;&lt; std::endl;
    return EXIT_FAILURE;
}

std::cout &lt;&lt; "Press enter to start SLAM ..." &lt;&lt; std::endl;
std::cin.get();

// start the SLAM
device-&gt;edge()-&gt;start();

std::cout &lt;&lt; "Press enter to stop SLAM ..." &lt;&lt; std::endl;
std::cin.get();

// to get the a color image in RGB format, can do the conversion with:
xv::RgbImage rgbImage = lastColorImage-&gt;toRgb();

// stop the slam
device-&gt;edge()-&gt;stop();

return EXIT_SUCCESS;
</pre><p> }</p>
<p>``` </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.9
</small></address>
</body>
</html>
